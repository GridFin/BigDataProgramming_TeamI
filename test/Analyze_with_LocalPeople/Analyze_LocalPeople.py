import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import warnings
import gc
import joblib
from datetime import datetime

warnings.filterwarnings("ignore")


def load_localpeople_quarterly_fixed(year, quarter, people_dir):
    """ìˆ˜ì •ëœ ë¶„ê¸°ë³„ LocalPeople ë°ì´í„° ë¡œë“œ"""

    quarter_months = {1: [1, 2, 3], 2: [4, 5, 6], 3: [7, 8, 9], 4: [10, 11, 12]}

    months = quarter_months[quarter]
    print(f"    {year}ë…„ {quarter}ë¶„ê¸° ì¸êµ¬ ë°ì´í„° ë¡œë“œ ì¤‘...")

    monthly_agg_list = []

    for month in months:
        month_str = f"{year}{month:02d}"
        file_path = os.path.join(people_dir, f"LOCAL_PEOPLE_DONG_{month_str}.csv")

        if not os.path.exists(file_path):
            continue

        try:
            # chunk ë‹¨ìœ„ë¡œ ì½ê¸°
            chunk_list = []
            chunksize = 50000

            # ğŸ”¥ CSV ì½ê¸° ë¬¸ì œ í•´ê²°: dtype ê°•ì œ ì§€ì • + BOM ì²˜ë¦¬
            expected_cols = [
                "ê¸°ì¤€ì¼ID",
                "ì‹œê°„ëŒ€êµ¬ë¶„",
                "í–‰ì •ë™ì½”ë“œ",
                "ì´ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì0ì„¸ë¶€í„°9ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì10ì„¸ë¶€í„°14ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì15ì„¸ë¶€í„°19ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì20ì„¸ë¶€í„°24ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì25ì„¸ë¶€í„°29ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì30ì„¸ë¶€í„°34ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì35ì„¸ë¶€í„°39ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì40ì„¸ë¶€í„°44ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì45ì„¸ë¶€í„°49ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì50ì„¸ë¶€í„°54ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì55ì„¸ë¶€í„°59ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì60ì„¸ë¶€í„°64ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì65ì„¸ë¶€í„°69ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì70ì„¸ì´ìƒìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì0ì„¸ë¶€í„°9ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì10ì„¸ë¶€í„°14ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì15ì„¸ë¶€í„°19ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì20ì„¸ë¶€í„°24ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì25ì„¸ë¶€í„°29ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì30ì„¸ë¶€í„°34ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì35ì„¸ë¶€í„°39ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì40ì„¸ë¶€í„°44ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì45ì„¸ë¶€í„°49ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì50ì„¸ë¶€í„°54ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì55ì„¸ë¶€í„°59ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì60ì„¸ë¶€í„°64ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì65ì„¸ë¶€í„°69ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì70ì„¸ì´ìƒìƒí™œì¸êµ¬ìˆ˜",
            ]

            try:
                df_iter = pd.read_csv(
                    file_path,
                    encoding="utf-8",
                    chunksize=chunksize,
                    dtype={
                        "ê¸°ì¤€ì¼ID": str,
                        "ì‹œê°„ëŒ€êµ¬ë¶„": str,
                        "í–‰ì •ë™ì½”ë“œ": str,
                    },  # í•µì‹¬: strë¡œ ê°•ì œ
                    usecols=expected_cols,  # ì •í™•í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                    header=0,  # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì„ì„ ëª…ì‹œ
                )
            except:
                df_iter = pd.read_csv(
                    file_path,
                    encoding="cp949",
                    chunksize=chunksize,
                    dtype={"ê¸°ì¤€ì¼ID": str, "ì‹œê°„ëŒ€êµ¬ë¶„": str, "í–‰ì •ë™ì½”ë“œ": str},
                    usecols=expected_cols,
                    header=0,
                )

            for chunk in df_iter:
                chunk["í–‰ì •ë™ì½”ë“œ"] = chunk["í–‰ì •ë™ì½”ë“œ"].astype(str).str.zfill(8)

                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                numeric_cols = ["ì´ìƒí™œì¸êµ¬ìˆ˜"] + [
                    col
                    for col in chunk.columns
                    if ("ë‚¨ì" in col or "ì—¬ì" in col) and "ìƒí™œì¸êµ¬ìˆ˜" in col
                ]
                keep_cols = ["í–‰ì •ë™ì½”ë“œ"] + numeric_cols
                chunk = chunk[keep_cols]

                # í–‰ì •ë™ë³„ ì§‘ê³„
                chunk_agg = (
                    chunk.groupby("í–‰ì •ë™ì½”ë“œ").sum(numeric_only=True).reset_index()
                )
                chunk_list.append(chunk_agg)

            # ì›”ë³„ ë°ì´í„° í•©ì¹˜ê¸°
            if chunk_list:
                month_df = pd.concat(chunk_list, ignore_index=True)
                month_agg = (
                    month_df.groupby("í–‰ì •ë™ì½”ë“œ").sum(numeric_only=True).reset_index()
                )
                monthly_agg_list.append(month_agg)
                print(f"      {month_str} ì™„ë£Œ ({len(month_agg)}ê°œ í–‰ì •ë™)")

            del chunk_list
            gc.collect()

        except Exception as e:
            print(f"      {month_str} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

    if not monthly_agg_list:
        return None

    # ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°
    quarter_df = monthly_agg_list[0]
    for month_df in monthly_agg_list[1:]:
        quarter_df = pd.merge(
            quarter_df,
            month_df,
            on="í–‰ì •ë™ì½”ë“œ",
            how="outer",
            suffixes=("", "_temp"),
        )

        # í‰ê·  ê³„ì‚°
        for col in month_df.columns:
            if col != "í–‰ì •ë™ì½”ë“œ":
                if f"{col}_temp" in quarter_df.columns:
                    quarter_df[col] = quarter_df[[col, f"{col}_temp"]].sum(
                        axis=1, skipna=True
                    )
                    quarter_df = quarter_df.drop(columns=[f"{col}_temp"])

    # ì´ì œ í–‰ì •ë™ì½”ë“œëŠ” ì´ë¯¸ ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ì— ìˆìŒ

    print(
        f"    {year}ë…„ {quarter}ë¶„ê¸° ì¸êµ¬ ë°ì´í„° ì§‘ê³„ ì™„ë£Œ: {len(quarter_df)}ê°œ í–‰ì •ë™"
    )
    return quarter_df


def load_trading_area_data(year, biz_dir):
    """Trading Area ë°ì´í„° ë¡œë“œ"""
    file_path = os.path.join(biz_dir, f"Trading_Area_{year}.csv")

    if not os.path.exists(file_path):
        return None

    try:
        try:
            df = pd.read_csv(file_path, encoding="utf-8")
        except:
            df = pd.read_csv(file_path, encoding="cp949")

        df = df.rename(columns={"í–‰ì •ë™_ì½”ë“œ": "í–‰ì •ë™ì½”ë“œ"})
        df["í–‰ì •ë™ì½”ë“œ"] = df["í–‰ì •ë™ì½”ë“œ"].astype(str).str.zfill(8)

        print(f"  Trading Area {year} ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")
        return df

    except Exception as e:
        print(f"  Trading Area {year} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def create_master_dataset_fixed(years, people_dir, biz_dir):
    """ìˆ˜ì •ëœ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ìƒì„±"""
    print("=== ìˆ˜ì •ëœ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ ===")

    all_data = []

    for year in years:
        print(f"\nğŸ“… {year}ë…„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

        # Trading Area ë°ì´í„° ë¡œë“œ
        biz_df = load_trading_area_data(year, biz_dir)
        if biz_df is None:
            continue

        # ê° ë¶„ê¸°ë³„ë¡œ ì²˜ë¦¬
        for quarter in [1, 2, 3, 4]:
            print(f"  ğŸ”„ {year}ë…„ {quarter}ë¶„ê¸° ì²˜ë¦¬ ì¤‘...")

            # í•´ë‹¹ ë¶„ê¸° Trading Area ë°ì´í„° í•„í„°ë§
            quarter_code = int(f"{year}{quarter}")
            biz_quarter = biz_df[biz_df["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ"] == quarter_code].copy()

            if len(biz_quarter) == 0:
                print(f"    âŒ {year}ë…„ {quarter}ë¶„ê¸° ë§¤ì¶œ ë°ì´í„° ì—†ìŒ")
                continue

            # LocalPeople ë°ì´í„° ë¡œë“œ (ìˆ˜ì •ëœ ë²„ì „)
            people_quarter = load_localpeople_quarterly_fixed(year, quarter, people_dir)

            if people_quarter is None:
                print(f"    âŒ {year}ë…„ {quarter}ë¶„ê¸° ì¸êµ¬ ë°ì´í„° ì—†ìŒ")
                continue

            # ë°ì´í„° ë³‘í•©
            merged_data = pd.merge(
                biz_quarter, people_quarter, on="í–‰ì •ë™ì½”ë“œ", how="left"
            )

            # ë³‘í•© ê²°ê³¼ í™•ì¸ (ì²« ë²ˆì§¸ ì¸êµ¬ ì»¬ëŸ¼ìœ¼ë¡œ ë§¤ì¹­ë¥  í™•ì¸)
            people_cols = [col for col in people_quarter.columns if col != "í–‰ì •ë™ì½”ë“œ"]
            if people_cols:
                people_match_rate = (
                    (~merged_data[people_cols[0]].isna()).sum() / len(merged_data) * 100
                )
            else:
                people_match_rate = 0

            print(
                f"    âœ… ë³‘í•© ì™„ë£Œ: ë§¤ì¶œ {len(biz_quarter)} + ì¸êµ¬ {len(people_quarter)} â†’ {len(merged_data)}í–‰"
            )
            print(f"    ğŸ“Š ì¸êµ¬ ë°ì´í„° ë§¤ì¹­ë¥ : {people_match_rate:.1f}%")

            all_data.append(merged_data)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del biz_quarter, people_quarter, merged_data
            gc.collect()

    if not all_data:
        raise ValueError("ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    # ì „ì²´ ë°ì´í„° ê²°í•©
    print("\nğŸ”— ì „ì²´ ë°ì´í„° ê²°í•© ì¤‘...")
    final_df = pd.concat(all_data, ignore_index=True)
    print(f"âœ… ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {final_df.shape}")

    return final_df


def preprocess_data_fixed(df):
    """ìˆ˜ì •ëœ ë°ì´í„° ì „ì²˜ë¦¬"""
    print("\n=== ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")

    # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
    if "ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡" not in df.columns:
        raise ValueError("íƒ€ê²Ÿ ë³€ìˆ˜ 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'ì´ ì—†ìŠµë‹ˆë‹¤!")

    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° + ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€
    drop_cols = ["í–‰ì •ë™_ì½”ë“œ_ëª…", "ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…"]

    # ğŸ”¥ ë§¤ì¶œ ê´€ë ¨ íŠ¹ì„± ì œê±° (ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€)
    sales_cols = [
        col
        for col in df.columns
        if any(
            keyword in col
            for keyword in [
                "ë§¤ì¶œ_ê¸ˆì•¡",
                "ë§¤ì¶œ_ê±´ìˆ˜",
                "ì›”ìš”ì¼_",
                "í™”ìš”ì¼_",
                "ìˆ˜ìš”ì¼_",
                "ëª©ìš”ì¼_",
                "ê¸ˆìš”ì¼_",
                "í† ìš”ì¼_",
                "ì¼ìš”ì¼_",
                "ì£¼ì¤‘_",
                "ì£¼ë§_",
                "ì‹œê°„ëŒ€_",
                "ë‚¨ì„±_ë§¤ì¶œ",
                "ì—¬ì„±_ë§¤ì¶œ",
                "ì—°ë ¹ëŒ€_",
            ]
        )
    ]

    # íƒ€ê²Ÿ ë³€ìˆ˜ëŠ” ìœ ì§€
    sales_cols = [col for col in sales_cols if col != "ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡"]

    print(f"ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°í•  ë§¤ì¶œ ê´€ë ¨ ì»¬ëŸ¼: {len(sales_cols)}ê°œ")
    print(f"ì œê±° ì»¬ëŸ¼ ì˜ˆì‹œ: {sales_cols[:5]}")

    drop_cols.extend(sales_cols)
    df = df.drop(columns=[col for col in drop_cols if col in df.columns])

    # ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ë§ì€ ì»¬ëŸ¼ ì œê±° (70% ì´ìƒ)
    null_ratio = df.isnull().mean()
    high_null_cols = null_ratio[null_ratio > 0.7].index.tolist()
    print(f"ê²°ì¸¡ë¥  70% ì´ìƒ ì»¬ëŸ¼ ì œê±°: {len(high_null_cols)}ê°œ")
    if high_null_cols:
        df = df.drop(columns=high_null_cols)

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    if "ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ" in df.columns:
        le = LabelEncoder()
        df["ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_encoded"] = le.fit_transform(
            df["ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ"].astype(str)
        )
        df = df.drop(columns=["ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ"])

    # LocalPeople ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
    people_cols = [
        col
        for col in df.columns
        if any(keyword in col for keyword in ["ìƒí™œì¸êµ¬ìˆ˜", "ë‚¨ì", "ì—¬ì"])
    ]
    print(f"ğŸ“Š LocalPeople ê´€ë ¨ ì»¬ëŸ¼: {len(people_cols)}ê°œ")

    if len(people_cols) == 0:
        print("âš ï¸ ê²½ê³ : LocalPeople ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    else:
        print("âœ… LocalPeople ë°ì´í„° í¬í•¨ í™•ì¸ë¨")
        # ëª‡ ê°œ ì»¬ëŸ¼ëª… ì¶œë ¥
        sample_cols = people_cols[:5]
        print(f"   ì˜ˆì‹œ: {sample_cols}")

    print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {df.shape}")
    return df


def train_and_evaluate_model_fixed(X_train, y_train, X_test, y_test):
    """ìˆ˜ì •ëœ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    print("\n=== ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ===")
    print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

    # íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜
    y_train_log = np.log1p(y_train.clip(lower=0))

    # RandomForest ëª¨ë¸
    model = RandomForestRegressor(
        n_estimators=100, max_depth=15, random_state=42, n_jobs=-1, oob_score=True
    )

    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    model.fit(X_train, y_train_log)
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

    # ì˜ˆì¸¡
    print("ğŸ”® ì˜ˆì¸¡ ì¤‘...")
    y_pred_log = model.predict(X_test)
    y_pred = np.expm1(y_pred_log)
    y_pred = np.clip(y_pred, 0, None)  # ìŒìˆ˜ ì œê±°

    # ì„±ëŠ¥ í‰ê°€
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)

    print("\n" + "=" * 60)
    print("ğŸ¯ ìˆ˜ì •ëœ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    print(f"MSE:  {mse:,.0f}")
    print(f"RMSE: {rmse:,.0f} ì›")
    print(f"MAE:  {mae:,.0f} ì›")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê·  ë§¤ì¶œ: {y_test.mean():,.0f} ì›")
    print(f"Out-of-Bag Score: {model.oob_score_:.4f}")

    # ìƒìœ„ 20ê°œ íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
    feature_importance = pd.DataFrame(
        {"feature": X_train.columns, "importance": model.feature_importances_}
    ).sort_values("importance", ascending=False)

    print(f"\n--- ìƒìœ„ 20ê°œ íŠ¹ì„± ì¤‘ìš”ë„ ---")
    for i, (_, row) in enumerate(feature_importance.head(20).iterrows(), 1):
        is_people = any(
            keyword in row["feature"] for keyword in ["ìƒí™œì¸êµ¬ìˆ˜", "ë‚¨ì", "ì—¬ì"]
        )
        marker = "ğŸ " if is_people else "ğŸª"
        print(f"{i:2d}. {marker} {row['feature'][:50]}: {row['importance']:.4f}")

    # LocalPeople ë°ì´í„° ê¸°ì—¬ë„
    people_features = feature_importance[
        feature_importance["feature"].str.contains("ìƒí™œì¸êµ¬ìˆ˜|ë‚¨ì|ì—¬ì", regex=True)
    ]
    people_importance = people_features["importance"].sum()

    print(
        f"\nğŸ“Š LocalPeople ë°ì´í„° ì´ ê¸°ì—¬ë„: {people_importance:.4f} ({people_importance*100:.1f}%)"
    )

    if people_importance < 0.05:
        print("âš ï¸ LocalPeople ë°ì´í„° ê¸°ì—¬ë„ê°€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.")
    elif people_importance < 0.15:
        print("ğŸ”¶ LocalPeople ë°ì´í„° ê¸°ì—¬ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.")
    else:
        print("âœ… LocalPeople ë°ì´í„°ê°€ ìœ ì˜ë¯¸í•˜ê²Œ ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

    return (
        model,
        {"mse": mse, "rmse": rmse, "mae": mae},
        feature_importance,
        people_importance,
    )


def save_model_and_metrics(
    model, metrics, feature_importance_df, people_importance, config
):
    """ëª¨ë¸ê³¼ í‰ê°€ì§€í‘œ ì €ì¥"""

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(script_dir, "results")
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
        print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±: {results_dir}")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. ëª¨ë¸ ì €ì¥
    model_filename = os.path.join(results_dir, f"localpeople_model_{timestamp}.joblib")
    joblib.dump(model, model_filename)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_filename}")

    # 2. í‰ê°€ì§€í‘œ CSV ì €ì¥
    metrics_data = {
        "ì‹¤í–‰ì‹œê°„": [timestamp],
        "MSE": [metrics["mse"]],
        "RMSE": [metrics["rmse"]],
        "MAE": [metrics["mae"]],
        "LocalPeople_ê¸°ì—¬ë„": [people_importance],
        "LocalPeople_ê¸°ì—¬ë„_í¼ì„¼íŠ¸": [people_importance * 100],
        "OOB_Score": [model.oob_score_],
        "í…ŒìŠ¤íŠ¸_ë…„ë„": [config["test_year"]],
        "í›ˆë ¨_ë°ì´í„°_í¬ê¸°": [config["train_size"]],
        "í…ŒìŠ¤íŠ¸_ë°ì´í„°_í¬ê¸°": [config["test_size"]],
        "íŠ¹ì„±_ê°œìˆ˜": [config["n_features"]],
    }

    metrics_df = pd.DataFrame(metrics_data)
    metrics_filename = os.path.join(results_dir, f"localpeople_metrics_{timestamp}.csv")
    metrics_df.to_csv(metrics_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ“Š í‰ê°€ì§€í‘œ CSV ì €ì¥ ì™„ë£Œ: {metrics_filename}")

    # 3. íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
    importance_filename = os.path.join(
        results_dir, f"localpeople_importance_{timestamp}.csv"
    )
    feature_importance_df.to_csv(importance_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ” íŠ¹ì„± ì¤‘ìš”ë„ CSV ì €ì¥ ì™„ë£Œ: {importance_filename}")

    # 4. ì‹¤í–‰ ì •ë³´ ìš”ì•½ ì €ì¥
    summary_data = {
        "í•­ëª©": [
            "ì‹¤í–‰ì‹œê°„",
            "MSE",
            "RMSE (ì›)",
            "MAE (ì›)",
            "LocalPeople ê¸°ì—¬ë„ (%)",
            "OOB Score",
            "í…ŒìŠ¤íŠ¸ ë…„ë„",
            "í›ˆë ¨ ë°ì´í„° í¬ê¸°",
            "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°",
            "íŠ¹ì„± ê°œìˆ˜",
        ],
        "ê°’": [
            timestamp,
            f"{metrics['mse']:,.0f}",
            f"{metrics['rmse']:,.0f}",
            f"{metrics['mae']:,.0f}",
            f"{people_importance*100:.1f}%",
            f"{model.oob_score_:.4f}",
            config["test_year"],
            f"{config['train_size']:,}",
            f"{config['test_size']:,}",
            config["n_features"],
        ],
    }

    summary_df = pd.DataFrame(summary_data)
    summary_filename = os.path.join(results_dir, f"localpeople_summary_{timestamp}.csv")
    summary_df.to_csv(summary_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ“‹ ëª¨ë¸ ìš”ì•½ CSV ì €ì¥ ì™„ë£Œ: {summary_filename}")

    print(f"\nğŸ‰ ëª¨ë“  ê²°ê³¼ê°€ '{results_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    return {
        "model_file": model_filename,
        "metrics_file": metrics_filename,
        "importance_file": importance_filename,
        "summary_file": summary_filename,
    }


def main():
    """main í•¨ìˆ˜"""
    print("ğŸš€ ìˆ˜ì •ëœ ìƒê¶Œ ë§¤ì¶œ ì˜ˆì¸¡ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸")
    print("=" * 50)

    # ì„¤ì •
    YEARS = range(2019, 2025)
    PEOPLE_DIR = "Data/LocalPeople"
    BIZ_DIR = "Data/Trading_Area"
    TEST_YEAR = 2024

    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° ë³‘í•© (ìˆ˜ì •ëœ ë²„ì „)
        master_df = create_master_dataset_fixed(YEARS, PEOPLE_DIR, BIZ_DIR)

        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        processed_df = preprocess_data_fixed(master_df)

        # 3. ë°ì´í„° ë¶„í• 
        print(f"\n=== ë°ì´í„° ë¶„í•  ({TEST_YEAR}ë…„ í…ŒìŠ¤íŠ¸) ===")
        train_mask = processed_df["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ"] < (TEST_YEAR * 10 + 1)
        test_mask = processed_df["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ"] >= (TEST_YEAR * 10 + 1)

        train_df = processed_df[train_mask].copy()
        test_df = processed_df[test_mask].copy()

        print(f"í›ˆë ¨ ë°ì´í„°: {len(train_df)}í–‰")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}í–‰")

        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        target_col = "ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡"
        feature_cols = [col for col in train_df.columns if col != target_col]

        X_train = train_df[feature_cols]
        y_train = train_df[target_col]
        X_test = test_df[feature_cols]
        y_test = test_df[target_col]

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        print("ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
        imputer = SimpleImputer(strategy="mean")
        X_train_filled = pd.DataFrame(
            imputer.fit_transform(X_train), columns=X_train.columns
        )
        X_test_filled = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

        # 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        model, metrics, feature_importance, people_importance = (
            train_and_evaluate_model_fixed(
                X_train_filled, y_train, X_test_filled, y_test
            )
        )

        print("\nğŸ‰ ìˆ˜ì •ëœ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print("=" * 50)

        # 5. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥
        config = {
            "test_year": TEST_YEAR,
            "train_size": len(train_df),
            "test_size": len(test_df),
            "n_features": len(feature_cols),
        }
        result = save_model_and_metrics(
            model, metrics, feature_importance, people_importance, config
        )

        print("\nğŸ‰ ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("=" * 50)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
