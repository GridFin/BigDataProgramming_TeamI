import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
import warnings
import gc
import joblib
from datetime import datetime

warnings.filterwarnings("ignore")


def load_localpeople_quarterly_fixed(year, quarter, people_dir):
    """ìˆ˜ì •ëœ ë¶„ê¸°ë³„ LocalPeople ë°ì´í„° ë¡œë“œ"""

    quarter_months = {1: [1, 2, 3], 2: [4, 5, 6], 3: [7, 8, 9], 4: [10, 11, 12]}

    months = quarter_months[quarter]
    print(f"    {year}ë…„ {quarter}ë¶„ê¸° ì¸êµ¬ ë°ì´í„° ë¡œë“œ ì¤‘...")

    monthly_agg_list = []

    for month in months:
        month_str = f"{year}{month:02d}"
        file_path = os.path.join(people_dir, f"LOCAL_PEOPLE_DONG_{month_str}.csv")

        if not os.path.exists(file_path):
            continue

        try:
            # chunk ë‹¨ìœ„ë¡œ ì½ê¸°
            chunk_list = []
            chunksize = 50000

            # ğŸ”¥ CSV ì½ê¸° ë¬¸ì œ í•´ê²°: dtype ê°•ì œ ì§€ì • + BOM ì²˜ë¦¬
            expected_cols = [
                "ê¸°ì¤€ì¼ID",
                "ì‹œê°„ëŒ€êµ¬ë¶„",
                "í–‰ì •ë™ì½”ë“œ",
                "ì´ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì0ì„¸ë¶€í„°9ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì10ì„¸ë¶€í„°14ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì15ì„¸ë¶€í„°19ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì20ì„¸ë¶€í„°24ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì25ì„¸ë¶€í„°29ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì30ì„¸ë¶€í„°34ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì35ì„¸ë¶€í„°39ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì40ì„¸ë¶€í„°44ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì45ì„¸ë¶€í„°49ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì50ì„¸ë¶€í„°54ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì55ì„¸ë¶€í„°59ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì60ì„¸ë¶€í„°64ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì65ì„¸ë¶€í„°69ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ë‚¨ì70ì„¸ì´ìƒìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì0ì„¸ë¶€í„°9ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì10ì„¸ë¶€í„°14ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì15ì„¸ë¶€í„°19ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì20ì„¸ë¶€í„°24ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì25ì„¸ë¶€í„°29ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì30ì„¸ë¶€í„°34ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì35ì„¸ë¶€í„°39ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì40ì„¸ë¶€í„°44ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì45ì„¸ë¶€í„°49ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì50ì„¸ë¶€í„°54ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì55ì„¸ë¶€í„°59ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì60ì„¸ë¶€í„°64ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì65ì„¸ë¶€í„°69ì„¸ìƒí™œì¸êµ¬ìˆ˜",
                "ì—¬ì70ì„¸ì´ìƒìƒí™œì¸êµ¬ìˆ˜",
            ]

            try:
                df_iter = pd.read_csv(
                    file_path,
                    encoding="utf-8",
                    chunksize=chunksize,
                    dtype={
                        "ê¸°ì¤€ì¼ID": str,
                        "ì‹œê°„ëŒ€êµ¬ë¶„": str,
                        "í–‰ì •ë™ì½”ë“œ": str,
                    },  # í•µì‹¬: strë¡œ ê°•ì œ
                    usecols=expected_cols,  # ì •í™•í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                    header=0,  # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì„ì„ ëª…ì‹œ
                )
            except:
                df_iter = pd.read_csv(
                    file_path,
                    encoding="cp949",
                    chunksize=chunksize,
                    dtype={"ê¸°ì¤€ì¼ID": str, "ì‹œê°„ëŒ€êµ¬ë¶„": str, "í–‰ì •ë™ì½”ë“œ": str},
                    usecols=expected_cols,
                    header=0,
                )

            for chunk in df_iter:
                chunk["í–‰ì •ë™ì½”ë“œ"] = chunk["í–‰ì •ë™ì½”ë“œ"].astype(str).str.zfill(8)

                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                numeric_cols = ["ì´ìƒí™œì¸êµ¬ìˆ˜"] + [
                    col
                    for col in chunk.columns
                    if ("ë‚¨ì" in col or "ì—¬ì" in col) and "ìƒí™œì¸êµ¬ìˆ˜" in col
                ]
                keep_cols = ["í–‰ì •ë™ì½”ë“œ"] + numeric_cols
                chunk = chunk[keep_cols]

                # í–‰ì •ë™ë³„ ì§‘ê³„
                chunk_agg = (
                    chunk.groupby("í–‰ì •ë™ì½”ë“œ").mean(numeric_only=True).reset_index()
                )
                chunk_list.append(chunk_agg)

            # ì›”ë³„ ë°ì´í„° í•©ì¹˜ê¸°
            if chunk_list:
                month_df = pd.concat(chunk_list, ignore_index=True)
                month_agg = (
                    month_df.groupby("í–‰ì •ë™ì½”ë“œ").mean(numeric_only=True).reset_index()
                )
                monthly_agg_list.append(month_agg)
                print(f"      {month_str} ì™„ë£Œ ({len(month_agg)}ê°œ í–‰ì •ë™)")

            del chunk_list
            gc.collect()

        except Exception as e:
            print(f"      {month_str} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

    if not monthly_agg_list:
        return None

    # ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°
    quarter_df = monthly_agg_list[0]
    for month_df in monthly_agg_list[1:]:
        quarter_df = pd.merge(
            quarter_df,
            month_df,
            on="í–‰ì •ë™ì½”ë“œ",
            how="outer",
            suffixes=("", "_temp"),
        )

        # í‰ê·  ê³„ì‚°
        for col in month_df.columns:
            if col != "í–‰ì •ë™ì½”ë“œ":
                if f"{col}_temp" in quarter_df.columns:
                    quarter_df[col] = quarter_df[[col, f"{col}_temp"]].mean(
                        axis=1, skipna=True
                    )
                    quarter_df = quarter_df.drop(columns=[f"{col}_temp"])

    print(
        f"    {year}ë…„ {quarter}ë¶„ê¸° ì¸êµ¬ ë°ì´í„° ì§‘ê³„ ì™„ë£Œ: {len(quarter_df)}ê°œ í–‰ì •ë™"
    )
    return quarter_df


def load_trading_area_data(year, biz_dir):
    """Trading Area ë°ì´í„° ë¡œë“œ"""
    file_path = os.path.join(biz_dir, f"Trading_Area_{year}.csv")

    if not os.path.exists(file_path):
        return None

    try:
        try:
            df = pd.read_csv(file_path, encoding="utf-8")
        except:
            df = pd.read_csv(file_path, encoding="cp949")

        df = df.rename(columns={"í–‰ì •ë™_ì½”ë“œ": "í–‰ì •ë™ì½”ë“œ"})
        df["í–‰ì •ë™ì½”ë“œ"] = df["í–‰ì •ë™ì½”ë“œ"].astype(str).str.zfill(8)

        print(f"  Trading Area {year} ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")
        return df

    except Exception as e:
        print(f"  Trading Area {year} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def create_master_dataset_fixed(years, people_dir, biz_dir):
    """ìˆ˜ì •ëœ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ìƒì„±"""
    print("=== ìˆ˜ì •ëœ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ ===")

    all_data = []

    for year in years:
        print(f"\nğŸ“… {year}ë…„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

        # Trading Area ë°ì´í„° ë¡œë“œ
        biz_df = load_trading_area_data(year, biz_dir)
        if biz_df is None:
            continue

        # ê° ë¶„ê¸°ë³„ë¡œ ì²˜ë¦¬
        for quarter in [1, 2, 3, 4]:
            print(f"  ğŸ”„ {year}ë…„ {quarter}ë¶„ê¸° ì²˜ë¦¬ ì¤‘...")

            # í•´ë‹¹ ë¶„ê¸° Trading Area ë°ì´í„° í•„í„°ë§
            quarter_code = int(f"{year}{quarter}")
            biz_quarter = biz_df[biz_df["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ"] == quarter_code].copy()

            if len(biz_quarter) == 0:
                print(f"    âŒ {year}ë…„ {quarter}ë¶„ê¸° ë§¤ì¶œ ë°ì´í„° ì—†ìŒ")
                continue

            # LocalPeople ë°ì´í„° ë¡œë“œ (ìˆ˜ì •ëœ ë²„ì „)
            people_quarter = load_localpeople_quarterly_fixed(year, quarter, people_dir)

            if people_quarter is None:
                print(f"    âŒ {year}ë…„ {quarter}ë¶„ê¸° ì¸êµ¬ ë°ì´í„° ì—†ìŒ")
                continue

            # ë°ì´í„° ë³‘í•©
            merged_data = pd.merge(
                biz_quarter, people_quarter, on="í–‰ì •ë™ì½”ë“œ", how="left"
            )

            # ë³‘í•© ê²°ê³¼ í™•ì¸ (ì²« ë²ˆì§¸ ì¸êµ¬ ì»¬ëŸ¼ìœ¼ë¡œ ë§¤ì¹­ë¥  í™•ì¸)
            people_cols = [col for col in people_quarter.columns if col != "í–‰ì •ë™ì½”ë“œ"]
            if people_cols:
                people_match_rate = (
                    (~merged_data[people_cols[0]].isna()).sum() / len(merged_data) * 100
                )
            else:
                people_match_rate = 0

            print(
                f"    âœ… ë³‘í•© ì™„ë£Œ: ë§¤ì¶œ {len(biz_quarter)} + ì¸êµ¬ {len(people_quarter)} â†’ {len(merged_data)}í–‰"
            )
            print(f"    ğŸ“Š ì¸êµ¬ ë°ì´í„° ë§¤ì¹­ë¥ : {people_match_rate:.1f}%")

            all_data.append(merged_data)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del biz_quarter, people_quarter, merged_data
            gc.collect()

    if not all_data:
        raise ValueError("ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    # ì „ì²´ ë°ì´í„° ê²°í•©
    print("\nğŸ”— ì „ì²´ ë°ì´í„° ê²°í•© ì¤‘...")
    final_df = pd.concat(all_data, ignore_index=True)
    print(f"âœ… ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {final_df.shape}")

    return final_df


def preprocess_data_fixed(df):
    """ìˆ˜ì •ëœ ë°ì´í„° ì „ì²˜ë¦¬"""
    print("\n=== ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")

    # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
    if "ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡" not in df.columns:
        raise ValueError("íƒ€ê²Ÿ ë³€ìˆ˜ 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'ì´ ì—†ìŠµë‹ˆë‹¤!")

    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° + ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€
    drop_cols = ["í–‰ì •ë™_ì½”ë“œ_ëª…", "ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…"]

    # ğŸ”¥ ë§¤ì¶œ ê´€ë ¨ íŠ¹ì„± ì œê±° (ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€)
    sales_cols = [
        col
        for col in df.columns
        if any(
            keyword in col
            for keyword in [
                "ë§¤ì¶œ_ê¸ˆì•¡",
                "ë§¤ì¶œ_ê±´ìˆ˜",
                "ì›”ìš”ì¼_",
                "í™”ìš”ì¼_",
                "ìˆ˜ìš”ì¼_",
                "ëª©ìš”ì¼_",
                "ê¸ˆìš”ì¼_",
                "í† ìš”ì¼_",
                "ì¼ìš”ì¼_",
                "ì£¼ì¤‘_",
                "ì£¼ë§_",
                "ì‹œê°„ëŒ€_",
                "ë‚¨ì„±_ë§¤ì¶œ",
                "ì—¬ì„±_ë§¤ì¶œ",
                "ì—°ë ¹ëŒ€_",
            ]
        )
    ]

    # íƒ€ê²Ÿ ë³€ìˆ˜ëŠ” ìœ ì§€
    sales_cols = [col for col in sales_cols if col != "ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡"]

    print(f"ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°í•  ë§¤ì¶œ ê´€ë ¨ ì»¬ëŸ¼: {len(sales_cols)}ê°œ")
    print(f"ì œê±° ì»¬ëŸ¼ ì˜ˆì‹œ: {sales_cols[:5]}")

    drop_cols.extend(sales_cols)
    df = df.drop(columns=[col for col in drop_cols if col in df.columns])

    # ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ë§ì€ ì»¬ëŸ¼ ì œê±° (70% ì´ìƒ)
    null_ratio = df.isnull().mean()
    high_null_cols = null_ratio[null_ratio > 0.7].index.tolist()
    print(f"ê²°ì¸¡ë¥  70% ì´ìƒ ì»¬ëŸ¼ ì œê±°: {len(high_null_cols)}ê°œ")
    if high_null_cols:
        df = df.drop(columns=high_null_cols)

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    if "ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ" in df.columns:
        le = LabelEncoder()
        df["ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_encoded"] = le.fit_transform(
            df["ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ"].astype(str)
        )
        df = df.drop(columns=["ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ"])

    # LocalPeople ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
    people_cols = [
        col
        for col in df.columns
        if any(keyword in col for keyword in ["ìƒí™œì¸êµ¬ìˆ˜", "ë‚¨ì", "ì—¬ì"])
    ]
    print(f"ğŸ“Š LocalPeople ê´€ë ¨ ì»¬ëŸ¼: {len(people_cols)}ê°œ")

    if len(people_cols) == 0:
        print("âš ï¸ ê²½ê³ : LocalPeople ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    else:
        print("âœ… LocalPeople ë°ì´í„° í¬í•¨ í™•ì¸ë¨")
        # ëª‡ ê°œ ì»¬ëŸ¼ëª… ì¶œë ¥
        sample_cols = people_cols[:5]
        print(f"   ì˜ˆì‹œ: {sample_cols}")

    print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {df.shape}")
    return df


def hyperparameter_tuning_and_predict(X_train, y_train, X_test, y_test, X_future=None):
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ëª¨ë¸ í•™ìŠµ í‰ê°€"""
    print("\n=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")
    print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

    # íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜
    y_train_log = np.log1p(y_train.clip(lower=0))
    y_test_log = np.log1p(y_test.clip(lower=0))

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
    param_grid = {
        "n_estimators": [200, 400, 800, 1200],
        "max_depth": [10, 20, 30, None],
        "min_samples_leaf": [1, 3, 10, 30],
        "max_features": ["auto", "sqrt", 0.3, 0.5, 0.8],
        "bootstrap": [True, False],
        "oob_score": [True],  # ê³ ì •
        "random_state": [42],  # ê³ ì •
        "n_jobs": [-1],  # ê³ ì •
    }

    # TimeSeriesSplit ì„¤ì • (5-fold)
    tscv = TimeSeriesSplit(n_splits=5)

    # RandomizedSearchCV ì„¤ì •
    rf_base = RandomForestRegressor()

    print("ğŸ” RandomizedSearchCV ì‹¤í–‰ ì¤‘...")
    print(f"   - íŒŒë¼ë¯¸í„° ì¡°í•© íƒìƒ‰: {50}ê°œ")
    print(f"   - CV í´ë“œ: {5}ê°œ")
    print(f"   - í‰ê°€ ì§€í‘œ: neg_mean_absolute_error")

    random_search = RandomizedSearchCV(
        estimator=rf_base,
        param_distributions=param_grid,
        n_iter=50,
        scoring="neg_mean_absolute_error",
        cv=tscv,
        n_jobs=-1,
        random_state=42,
        verbose=1,
    )

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰
    random_search.fit(X_train, y_train_log)

    print("âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ† ë² ìŠ¤íŠ¸ CV ìŠ¤ì½”ì–´: {-random_search.best_score_:.4f}")
    print("ğŸ¯ ë² ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°:")
    for param, value in random_search.best_params_.items():
        print(f"   - {param}: {value}")

    # ë² ìŠ¤íŠ¸ ëª¨ë¸ë¡œ ì˜ˆì¸¡
    best_model = random_search.best_estimator_

    print("\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    y_pred_log = best_model.predict(X_test)
    y_pred = np.expm1(y_pred_log)
    y_pred = np.clip(y_pred, 0, None)  # ìŒìˆ˜ ì œê±°

    # ì„±ëŠ¥ í‰ê°€
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)

    print("\n" + "=" * 70)
    print("ğŸ¯ LocalPeople í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    print("=" * 70)
    print(f"MSE:  {mse:,.0f}")
    print(f"RMSE: {rmse:,.0f} ì›")
    print(f"MAE:  {mae:,.0f} ì›")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê·  ë§¤ì¶œ: {y_test.mean():,.0f} ì›")
    print(f"ë² ìŠ¤íŠ¸ CV MAE: {-random_search.best_score_:,.0f} ì›")
    print(f"Out-of-Bag Score: {best_model.oob_score_:.4f}")

    # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    feature_importance = pd.DataFrame(
        {"feature": X_train.columns, "importance": best_model.feature_importances_}
    ).sort_values("importance", ascending=False)

    print(f"\n--- ìƒìœ„ 20ê°œ íŠ¹ì„± ì¤‘ìš”ë„ ---")
    for i, (_, row) in enumerate(feature_importance.head(20).iterrows(), 1):
        is_people = any(
            keyword in row["feature"] for keyword in ["ìƒí™œì¸êµ¬ìˆ˜", "ë‚¨ì", "ì—¬ì"]
        )
        marker = "ğŸ " if is_people else "ğŸª"
        print(f"{i:2d}. {marker} {row['feature'][:50]}: {row['importance']:.4f}")

    # LocalPeople ë°ì´í„° ê¸°ì—¬ë„
    people_features = feature_importance[
        feature_importance["feature"].str.contains("ìƒí™œì¸êµ¬ìˆ˜|ë‚¨ì|ì—¬ì", regex=True)
    ]
    people_importance = people_features["importance"].sum()

    print(
        f"\nğŸ“Š LocalPeople ë°ì´í„° ì´ ê¸°ì—¬ë„: {people_importance:.4f} ({people_importance*100:.1f}%)"
    )

    # 2025ë…„ ì˜ˆì¸¡ (ë§Œì•½ ë°ì´í„°ê°€ ìˆë‹¤ë©´)
    future_predictions = None
    if X_future is not None and len(X_future) > 0:
        print("\nğŸ”® 2025ë…„ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
        y_future_log = best_model.predict(X_future)
        future_predictions = np.expm1(y_future_log)
        future_predictions = np.clip(future_predictions, 0, None)
        print(f"âœ… 2025ë…„ ì˜ˆì¸¡ ì™„ë£Œ: {len(future_predictions)}ê°œ ì˜ˆì¸¡ê°’")
        print(f"   í‰ê·  ì˜ˆì¸¡ ë§¤ì¶œ: {future_predictions.mean():,.0f} ì›")

    return (
        best_model,
        {"mse": mse, "rmse": rmse, "mae": mae, "cv_mae": -random_search.best_score_},
        feature_importance,
        people_importance,
        random_search.best_params_,
        future_predictions,
    )


def save_model_and_metrics(
    model,
    metrics,
    feature_importance_df,
    people_importance,
    best_params,
    config,
    future_predictions=None,
    future_data=None,
):
    """ëª¨ë¸ê³¼ í‰ê°€ì§€í‘œ ì €ì¥"""

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(script_dir, "results")
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
        print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±: {results_dir}")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. ëª¨ë¸ ì €ì¥
    model_filename = os.path.join(
        results_dir, f"localpeople_tuned_model_{timestamp}.joblib"
    )
    joblib.dump(model, model_filename)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_filename}")

    # 2. í‰ê°€ì§€í‘œ CSV ì €ì¥
    metrics_data = {
        "ì‹¤í–‰ì‹œê°„": [timestamp],
        "MSE": [metrics["mse"]],
        "RMSE": [metrics["rmse"]],
        "MAE": [metrics["mae"]],
        "CV_MAE": [metrics["cv_mae"]],
        "LocalPeople_ê¸°ì—¬ë„": [people_importance],
        "LocalPeople_ê¸°ì—¬ë„_í¼ì„¼íŠ¸": [people_importance * 100],
        "OOB_Score": [model.oob_score_],
        "í…ŒìŠ¤íŠ¸_ë…„ë„": [config["test_year"]],
        "í›ˆë ¨_ë°ì´í„°_í¬ê¸°": [config["train_size"]],
        "í…ŒìŠ¤íŠ¸_ë°ì´í„°_í¬ê¸°": [config["test_size"]],
        "íŠ¹ì„±_ê°œìˆ˜": [config["n_features"]],
    }

    metrics_df = pd.DataFrame(metrics_data)
    metrics_filename = os.path.join(
        results_dir, f"localpeople_tuned_metrics_{timestamp}.csv"
    )
    metrics_df.to_csv(metrics_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ“Š í‰ê°€ì§€í‘œ CSV ì €ì¥ ì™„ë£Œ: {metrics_filename}")

    # 3. ë² ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ì €ì¥
    params_df = pd.DataFrame([best_params])
    params_filename = os.path.join(
        results_dir, f"localpeople_best_params_{timestamp}.csv"
    )
    params_df.to_csv(params_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ¯ ë² ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° CSV ì €ì¥ ì™„ë£Œ: {params_filename}")

    # 4. íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
    importance_filename = os.path.join(
        results_dir, f"localpeople_tuned_importance_{timestamp}.csv"
    )
    feature_importance_df.to_csv(importance_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ” íŠ¹ì„± ì¤‘ìš”ë„ CSV ì €ì¥ ì™„ë£Œ: {importance_filename}")

    # 5. 2025ë…„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    if future_predictions is not None and future_data is not None:
        future_df = future_data.copy()
        future_df["ì˜ˆì¸¡_ë§¤ì¶œ_ê¸ˆì•¡"] = future_predictions
        future_filename = os.path.join(
            results_dir, f"localpeople_2025_predictions_{timestamp}.csv"
        )
        future_df.to_csv(future_filename, index=False, encoding="utf-8-sig")
        print(f"ğŸ”® 2025ë…„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {future_filename}")

    # 6. ì‹¤í–‰ ì •ë³´ ìš”ì•½ ì €ì¥
    summary_data = {
        "í•­ëª©": [
            "ì‹¤í–‰ì‹œê°„",
            "MSE",
            "RMSE (ì›)",
            "MAE (ì›)",
            "CV MAE (ì›)",
            "LocalPeople ê¸°ì—¬ë„ (%)",
            "OOB Score",
            "í…ŒìŠ¤íŠ¸ ë…„ë„",
            "í›ˆë ¨ ë°ì´í„° í¬ê¸°",
            "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°",
            "íŠ¹ì„± ê°œìˆ˜",
        ],
        "ê°’": [
            timestamp,
            f"{metrics['mse']:,.0f}",
            f"{metrics['rmse']:,.0f}",
            f"{metrics['mae']:,.0f}",
            f"{metrics['cv_mae']:,.0f}",
            f"{people_importance*100:.1f}%",
            f"{model.oob_score_:.4f}",
            config["test_year"],
            f"{config['train_size']:,}",
            f"{config['test_size']:,}",
            config["n_features"],
        ],
    }

    summary_df = pd.DataFrame(summary_data)
    summary_filename = os.path.join(
        results_dir, f"localpeople_tuned_summary_{timestamp}.csv"
    )
    summary_df.to_csv(summary_filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ“‹ ëª¨ë¸ ìš”ì•½ CSV ì €ì¥ ì™„ë£Œ: {summary_filename}")

    print(f"\nğŸ‰ ëª¨ë“  ê²°ê³¼ê°€ '{results_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    return {
        "model_file": model_filename,
        "metrics_file": metrics_filename,
        "params_file": params_filename,
        "importance_file": importance_filename,
        "summary_file": summary_filename,
        "future_file": future_filename if future_predictions is not None else None,
    }


def main():
    """main í•¨ìˆ˜"""
    print("ğŸš€ LocalPeople í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìƒê¶Œ ë§¤ì¶œ ì˜ˆì¸¡ ëª¨ë¸")
    print("=" * 60)

    # ì„¤ì •
    YEARS = range(2019, 2025)
    PEOPLE_DIR = "Data/LocalPeople"
    BIZ_DIR = "Data/Trading_Area"
    TEST_YEAR = 2024
    FUTURE_YEAR = 2025

    try:
        # 1. 2019-2024 ë°ì´í„° ë¡œë“œ ë° ë³‘í•©
        master_df = create_master_dataset_fixed(YEARS, PEOPLE_DIR, BIZ_DIR)

        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        processed_df = preprocess_data_fixed(master_df)

        # 3. ë°ì´í„° ë¶„í• 
        print(f"\n=== ë°ì´í„° ë¶„í•  ({TEST_YEAR}ë…„ í…ŒìŠ¤íŠ¸) ===")
        train_mask = processed_df["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ"] < (TEST_YEAR * 10 + 1)
        test_mask = processed_df["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ"] >= (TEST_YEAR * 10 + 1)

        train_df = processed_df[train_mask].copy()
        test_df = processed_df[test_mask].copy()

        print(f"í›ˆë ¨ ë°ì´í„°: {len(train_df)}í–‰")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}í–‰")

        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        target_col = "ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡"
        feature_cols = [col for col in train_df.columns if col != target_col]

        X_train = train_df[feature_cols]
        y_train = train_df[target_col]
        X_test = test_df[feature_cols]
        y_test = test_df[target_col]

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        print("ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
        imputer = SimpleImputer(strategy="mean")
        X_train_filled = pd.DataFrame(
            imputer.fit_transform(X_train), columns=X_train.columns
        )
        X_test_filled = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

        # 4. 2025ë…„ ë°ì´í„° ì¤€ë¹„ (ìˆë‹¤ë©´)
        print(f"\n=== {FUTURE_YEAR}ë…„ ë°ì´í„° í™•ì¸ ===")
        future_df = None
        X_future_filled = None

        try:
            future_master = create_master_dataset_fixed(
                [FUTURE_YEAR], PEOPLE_DIR, BIZ_DIR
            )
            if future_master is not None and len(future_master) > 0:
                future_processed = preprocess_data_fixed(future_master)

                # 2025ë…„ ë°ì´í„°ì—ì„œ íŠ¹ì„±ë§Œ ì¶”ì¶œ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
                future_features = [
                    col for col in future_processed.columns if col in feature_cols
                ]
                X_future = future_processed[future_features]
                X_future_filled = pd.DataFrame(
                    imputer.transform(X_future), columns=X_future.columns
                )
                future_df = future_processed.copy()

                print(f"âœ… {FUTURE_YEAR}ë…„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(X_future_filled)}í–‰")
            else:
                print(f"âŒ {FUTURE_YEAR}ë…„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ {FUTURE_YEAR}ë…„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ëª¨ë¸ í•™ìŠµ
        (
            model,
            metrics,
            feature_importance,
            people_importance,
            best_params,
            future_predictions,
        ) = hyperparameter_tuning_and_predict(
            X_train_filled, y_train, X_test_filled, y_test, X_future_filled
        )

        print("\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print("=" * 60)

        # 6. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥
        config = {
            "test_year": TEST_YEAR,
            "train_size": len(train_df),
            "test_size": len(test_df),
            "n_features": len(feature_cols),
        }

        result = save_model_and_metrics(
            model,
            metrics,
            feature_importance,
            people_importance,
            best_params,
            config,
            future_predictions,
            future_df,
        )

        print("\nğŸ‰ ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("=" * 60)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
