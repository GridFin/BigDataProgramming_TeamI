{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7890dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas>=1.5.0 (from -r ../../requirements.txt (line 2))\n",
      "  Using cached pandas-2.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.21.0 (from -r ../../requirements.txt (line 3))\n",
      "  Using cached numpy-2.3.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from -r ../../requirements.txt (line 6))\n",
      "  Using cached scikit_learn-1.7.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting joblib>=1.2.0 (from -r ../../requirements.txt (line 7))\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting matplotlib>=3.5.0 (from -r ../../requirements.txt (line 11))\n",
      "  Using cached matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn>=0.11.0 (from -r ../../requirements.txt (line 12))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting jupyter>=1.0.0 (from -r ../../requirements.txt (line 15))\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting notebook>=6.4.0 (from -r ../../requirements.txt (line 16))\n",
      "  Using cached notebook-7.4.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting catboost>=1.1.0 (from -r ../../requirements.txt (line 19))\n",
      "  Using cached catboost-1.2.8-cp312-cp312-macosx_11_0_universal2.whl.metadata (1.4 kB)\n",
      "Collecting plotly>=5.0.0 (from -r ../../requirements.txt (line 22))\n",
      "  Using cached plotly-6.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from pandas>=1.5.0->-r ../../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.5.0->-r ../../requirements.txt (line 2))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.5.0->-r ../../requirements.txt (line 2))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn>=1.1.0->-r ../../requirements.txt (line 6))\n",
      "  Using cached scipy-1.15.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->-r ../../requirements.txt (line 6))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.5.0->-r ../../requirements.txt (line 11))\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.5.0->-r ../../requirements.txt (line 11))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.5.0->-r ../../requirements.txt (line 11))\n",
      "  Using cached fonttools-4.58.4-cp312-cp312-macosx_10_13_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.5.0->-r ../../requirements.txt (line 11))\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.0->-r ../../requirements.txt (line 11)) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib>=3.5.0->-r ../../requirements.txt (line 11))\n",
      "  Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.5.0->-r ../../requirements.txt (line 11))\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting jupyter-console (from jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (6.29.5)\n",
      "Collecting ipywidgets (from jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jupyterlab (from jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2 (from notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from notebook>=6.4.0->-r ../../requirements.txt (line 16)) (6.5.1)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jinja2>=3.0.3 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16)) (5.8.1)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16)) (27.0.0)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16)) (5.14.3)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx>=0.25.0 (from jupyterlab->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting setuptools>=41.1.0 (from jupyterlab->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting requests>=2.31 (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting graphviz (from catboost>=1.1.0->-r ../../requirements.txt (line 19))\n",
      "  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: six in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from catboost>=1.1.0->-r ../../requirements.txt (line 19)) (1.17.0)\n",
      "Collecting narwhals>=1.15.1 (from plotly>=5.0.0->-r ../../requirements.txt (line 22))\n",
      "  Using cached narwhals-1.43.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting idna>=2.8 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing_extensions>=4.5 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (6.7 kB)\n",
      "Collecting certifi (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: appnope in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (9.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (7.0.0)\n",
      "Requirement already satisfied: decorator in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.8.4)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached rpds_py-0.25.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16)) (4.3.8)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pyyaml>=5.3 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.7.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets->jupyter>=1.0.0->-r ../../requirements.txt (line 15))\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r ../../requirements.txt (line 16))\n",
      "  Using cached types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../../requirements.txt (line 15)) (0.2.3)\n",
      "Using cached pandas-2.3.0-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached numpy-2.3.0-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached scikit_learn-1.7.0-cp312-cp312-macosx_12_0_arm64.whl (10.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Using cached notebook-7.4.3-py3-none-any.whl (14.3 MB)\n",
      "Using cached jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "Using cached jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)\n",
      "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached catboost-1.2.8-cp312-cp312-macosx_11_0_universal2.whl (27.8 MB)\n",
      "Using cached plotly-6.1.2-py3-none-any.whl (16.3 MB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.4-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Using cached jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached narwhals-1.43.1-py3-none-any.whl (362 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached rpds_py-0.25.1-cp312-cp312-macosx_11_0_arm64.whl (350 kB)\n",
      "Using cached scipy-1.15.3-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl (53 kB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, pytz, fastjsonschema, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, tzdata, typing_extensions, types-python-dateutil, tinycss2, threadpoolctl, terminado, soupsieve, sniffio, setuptools, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, pyyaml, python-json-logger, pyparsing, pycparser, prometheus-client, pillow, pandocfilters, overrides, numpy, narwhals, mistune, MarkupSafe, kiwisolver, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, joblib, idna, h11, graphviz, fqdn, fonttools, defusedxml, cycler, charset_normalizer, certifi, bleach, babel, attrs, async-lru, scipy, requests, referencing, plotly, pandas, jupyter-server-terminals, jinja2, httpcore, contourpy, cffi, beautifulsoup4, arrow, anyio, scikit-learn, matplotlib, jsonschema-specifications, isoduration, ipywidgets, httpx, argon2-cffi-bindings, seaborn, jupyter-console, jsonschema, catboost, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88/88\u001b[0m [jupyter][notebook]jupyterlab]server]minals]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 anyio-4.9.0 argon2-cffi-25.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.5 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.13.4 bleach-6.2.0 catboost-1.2.8 certifi-2025.6.15 cffi-1.17.1 charset_normalizer-3.4.2 contourpy-1.3.2 cycler-0.12.1 defusedxml-0.7.1 fastjsonschema-2.21.1 fonttools-4.58.4 fqdn-1.5.1 graphviz-0.21 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 ipywidgets-8.1.7 isoduration-20.11.0 jinja2-3.1.6 joblib-1.5.1 json5-0.12.0 jsonpointer-3.0.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab_widgets-3.0.15 kiwisolver-1.4.8 matplotlib-3.10.3 mistune-3.1.3 narwhals-1.43.1 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.4.3 notebook-shim-0.2.4 numpy-2.3.0 overrides-7.7.0 pandas-2.3.0 pandocfilters-1.5.1 pillow-11.2.1 plotly-6.1.2 prometheus-client-0.22.1 pycparser-2.22 pyparsing-3.2.3 python-json-logger-3.3.0 pytz-2025.2 pyyaml-6.0.2 referencing-0.36.2 requests-2.32.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.25.1 scikit-learn-1.7.0 scipy-1.15.3 seaborn-0.13.2 send2trash-1.8.3 setuptools-80.9.0 sniffio-1.3.1 soupsieve-2.7 terminado-0.18.1 threadpoolctl-3.6.0 tinycss2-1.4.0 types-python-dateutil-2.9.0.20250516 typing_extensions-4.14.0 tzdata-2025.2 uri-template-1.3.0 urllib3-2.5.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "# 패키지를 설치 안 했다면 아래 명령어를 주석을 해제하고 실행시키세요.\n",
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9e5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 LocalPeople 폴더가 없습니다. 다운로드 및 압축 해제 시작...\n",
      "🔽 LocalPeople.zip 다운로드 중...\n",
      "✅ 다운로드 완료!\n",
      "📦 압축 해제 중...\n",
      "✅ 압축 해제 완료!\n",
      "🧹 ZIP 파일 삭제 완료!\n"
     ]
    }
   ],
   "source": [
    "# 실거주 인구 데이터 다운로드\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "# 경로 설정\n",
    "base_dir = \"../../Data\"\n",
    "localpeople_dir = os.path.join(base_dir, \"LocalPeople\")\n",
    "zip_url = (\n",
    "    \"https://huggingface.co/datasets/uhjin1130/LocalPeople/resolve/main/LocalPeople.zip\"\n",
    ")\n",
    "zip_path = os.path.join(localpeople_dir, \"LocalPeople.zip\")\n",
    "\n",
    "# LocalPeople 폴더가 없으면 생성 및 다운로드\n",
    "if not os.path.exists(localpeople_dir):\n",
    "    print(\"📁 LocalPeople 폴더가 없습니다. 다운로드 및 압축 해제 시작...\")\n",
    "\n",
    "    os.makedirs(localpeople_dir, exist_ok=True)\n",
    "\n",
    "    # 다운로드\n",
    "    print(\"🔽 LocalPeople.zip 다운로드 중...\")\n",
    "    response = requests.get(zip_url)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"✅ 다운로드 완료!\")\n",
    "\n",
    "    # 압축 해제 (해당 폴더 안으로)\n",
    "    print(\"📦 압축 해제 중...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(localpeople_dir)\n",
    "    print(\"✅ 압축 해제 완료!\")\n",
    "\n",
    "    # 압축 파일 삭제\n",
    "    os.remove(zip_path)\n",
    "    print(\"🧹 ZIP 파일 삭제 완료!\")\n",
    "\n",
    "else:\n",
    "    print(\"✅ LocalPeople 폴더가 이미 존재합니다. 다운로드 생략.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40dbeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import gc\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6241a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_localpeople_quarterly_fixed(year, quarter, people_dir):\n",
    "    \"\"\"수정된 분기별 LocalPeople 데이터 로드\"\"\"\n",
    "\n",
    "    quarter_months = {1: [1, 2, 3], 2: [4, 5, 6], 3: [7, 8, 9], 4: [10, 11, 12]}\n",
    "\n",
    "    months = quarter_months[quarter]\n",
    "    print(f\"    {year}년 {quarter}분기 인구 데이터 로드 중...\")\n",
    "\n",
    "    monthly_agg_list = []\n",
    "\n",
    "    for month in months:\n",
    "        month_str = f\"{year}{month:02d}\"\n",
    "        file_path = os.path.join(people_dir, f\"LOCAL_PEOPLE_DONG_{month_str}.csv\")\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # chunk 단위로 읽기\n",
    "            chunk_list = []\n",
    "            chunksize = 50000\n",
    "\n",
    "            # 🔥 CSV 읽기 문제 해결: dtype 강제 지정 + BOM 처리\n",
    "            expected_cols = [\n",
    "                \"기준일ID\",\n",
    "                \"시간대구분\",\n",
    "                \"행정동코드\",\n",
    "                \"총생활인구수\",\n",
    "                \"남자0세부터9세생활인구수\",\n",
    "                \"남자10세부터14세생활인구수\",\n",
    "                \"남자15세부터19세생활인구수\",\n",
    "                \"남자20세부터24세생활인구수\",\n",
    "                \"남자25세부터29세생활인구수\",\n",
    "                \"남자30세부터34세생활인구수\",\n",
    "                \"남자35세부터39세생활인구수\",\n",
    "                \"남자40세부터44세생활인구수\",\n",
    "                \"남자45세부터49세생활인구수\",\n",
    "                \"남자50세부터54세생활인구수\",\n",
    "                \"남자55세부터59세생활인구수\",\n",
    "                \"남자60세부터64세생활인구수\",\n",
    "                \"남자65세부터69세생활인구수\",\n",
    "                \"남자70세이상생활인구수\",\n",
    "                \"여자0세부터9세생활인구수\",\n",
    "                \"여자10세부터14세생활인구수\",\n",
    "                \"여자15세부터19세생활인구수\",\n",
    "                \"여자20세부터24세생활인구수\",\n",
    "                \"여자25세부터29세생활인구수\",\n",
    "                \"여자30세부터34세생활인구수\",\n",
    "                \"여자35세부터39세생활인구수\",\n",
    "                \"여자40세부터44세생활인구수\",\n",
    "                \"여자45세부터49세생활인구수\",\n",
    "                \"여자50세부터54세생활인구수\",\n",
    "                \"여자55세부터59세생활인구수\",\n",
    "                \"여자60세부터64세생활인구수\",\n",
    "                \"여자65세부터69세생활인구수\",\n",
    "                \"여자70세이상생활인구수\",\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                df_iter = pd.read_csv(\n",
    "                    file_path,\n",
    "                    encoding=\"utf-8\",\n",
    "                    chunksize=chunksize,\n",
    "                    dtype={\n",
    "                        \"기준일ID\": str,\n",
    "                        \"시간대구분\": str,\n",
    "                        \"행정동코드\": str,\n",
    "                    },  # 핵심: str로 강제\n",
    "                    usecols=expected_cols,  # 정확한 컬럼만 선택\n",
    "                    header=0,  # 첫 번째 행이 헤더임을 명시\n",
    "                )\n",
    "            except:\n",
    "                df_iter = pd.read_csv(\n",
    "                    file_path,\n",
    "                    encoding=\"cp949\",\n",
    "                    chunksize=chunksize,\n",
    "                    dtype={\"기준일ID\": str, \"시간대구분\": str, \"행정동코드\": str},\n",
    "                    usecols=expected_cols,\n",
    "                    header=0,\n",
    "                )\n",
    "\n",
    "            for chunk in df_iter:\n",
    "                chunk[\"행정동코드\"] = chunk[\"행정동코드\"].astype(str).str.zfill(8)\n",
    "\n",
    "                # 필요한 컬럼만 선택\n",
    "                numeric_cols = [\"총생활인구수\"] + [\n",
    "                    col\n",
    "                    for col in chunk.columns\n",
    "                    if (\"남자\" in col or \"여자\" in col) and \"생활인구수\" in col\n",
    "                ]\n",
    "                keep_cols = [\"행정동코드\"] + numeric_cols\n",
    "                chunk = chunk[keep_cols]\n",
    "\n",
    "                # 행정동별 집계\n",
    "                chunk_agg = (\n",
    "                    chunk.groupby(\"행정동코드\").sum(numeric_only=True).reset_index()\n",
    "                )\n",
    "                chunk_list.append(chunk_agg)\n",
    "\n",
    "            # 월별 데이터 합치기\n",
    "            if chunk_list:\n",
    "                month_df = pd.concat(chunk_list, ignore_index=True)\n",
    "                month_agg = (\n",
    "                    month_df.groupby(\"행정동코드\").sum(numeric_only=True).reset_index()\n",
    "                )\n",
    "                monthly_agg_list.append(month_agg)\n",
    "                print(f\"      {month_str} 완료 ({len(month_agg)}개 행정동)\")\n",
    "\n",
    "            del chunk_list\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      {month_str} 로드 실패: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not monthly_agg_list:\n",
    "        return None\n",
    "\n",
    "    # 분기별 평균 계산\n",
    "    quarter_df = monthly_agg_list[0]\n",
    "    for month_df in monthly_agg_list[1:]:\n",
    "        quarter_df = pd.merge(\n",
    "            quarter_df,\n",
    "            month_df,\n",
    "            on=\"행정동코드\",\n",
    "            how=\"outer\",\n",
    "            suffixes=(\"\", \"_temp\"),\n",
    "        )\n",
    "\n",
    "        # 평균 계산\n",
    "        for col in month_df.columns:\n",
    "            if col != \"행정동코드\":\n",
    "                if f\"{col}_temp\" in quarter_df.columns:\n",
    "                    quarter_df[col] = quarter_df[[col, f\"{col}_temp\"]].sum(\n",
    "                        axis=1, skipna=True\n",
    "                    )\n",
    "                    quarter_df = quarter_df.drop(columns=[f\"{col}_temp\"])\n",
    "\n",
    "    # 이제 행정동코드는 이미 올바른 컬럼에 있음\n",
    "\n",
    "    print(\n",
    "        f\"    {year}년 {quarter}분기 인구 데이터 집계 완료: {len(quarter_df)}개 행정동\"\n",
    "    )\n",
    "    return quarter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605181e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trading_area_data(year, biz_dir):\n",
    "    \"\"\"Trading Area 데이터 로드\"\"\"\n",
    "    file_path = os.path.join(biz_dir, f\"Trading_Area_{year}.csv\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        except:\n",
    "            df = pd.read_csv(file_path, encoding=\"cp949\")\n",
    "\n",
    "        df = df.rename(columns={\"행정동_코드\": \"행정동코드\"})\n",
    "        df[\"행정동코드\"] = df[\"행정동코드\"].astype(str).str.zfill(8)\n",
    "\n",
    "        print(f\"  Trading Area {year} 로드 완료: {len(df)}행\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Trading Area {year} 로드 실패: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0d2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_dataset_fixed(years, people_dir, biz_dir):\n",
    "    \"\"\"수정된 마스터 데이터셋 생성\"\"\"\n",
    "    print(\"=== 수정된 마스터 데이터셋 생성 시작 ===\")\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"\\n📅 {year}년 데이터 처리 중...\")\n",
    "\n",
    "        # Trading Area 데이터 로드\n",
    "        biz_df = load_trading_area_data(year, biz_dir)\n",
    "        if biz_df is None:\n",
    "            continue\n",
    "\n",
    "        # 각 분기별로 처리\n",
    "        for quarter in [1, 2, 3, 4]:\n",
    "            print(f\"  🔄 {year}년 {quarter}분기 처리 중...\")\n",
    "\n",
    "            # 해당 분기 Trading Area 데이터 필터링\n",
    "            quarter_code = int(f\"{year}{quarter}\")\n",
    "            biz_quarter = biz_df[biz_df[\"기준_년분기_코드\"] == quarter_code].copy()\n",
    "\n",
    "            if len(biz_quarter) == 0:\n",
    "                print(f\"    ❌ {year}년 {quarter}분기 매출 데이터 없음\")\n",
    "                continue\n",
    "\n",
    "            # LocalPeople 데이터 로드 (수정된 버전)\n",
    "            people_quarter = load_localpeople_quarterly_fixed(year, quarter, people_dir)\n",
    "\n",
    "            if people_quarter is None:\n",
    "                print(f\"    ❌ {year}년 {quarter}분기 인구 데이터 없음\")\n",
    "                continue\n",
    "\n",
    "            # 데이터 병합\n",
    "            merged_data = pd.merge(\n",
    "                biz_quarter, people_quarter, on=\"행정동코드\", how=\"left\"\n",
    "            )\n",
    "\n",
    "            # 병합 결과 확인 (첫 번째 인구 컬럼으로 매칭률 확인)\n",
    "            people_cols = [col for col in people_quarter.columns if col != \"행정동코드\"]\n",
    "            if people_cols:\n",
    "                people_match_rate = (\n",
    "                    (~merged_data[people_cols[0]].isna()).sum() / len(merged_data) * 100\n",
    "                )\n",
    "            else:\n",
    "                people_match_rate = 0\n",
    "\n",
    "            print(\n",
    "                f\"    ✅ 병합 완료: 매출 {len(biz_quarter)} + 인구 {len(people_quarter)} → {len(merged_data)}행\"\n",
    "            )\n",
    "            print(f\"    📊 인구 데이터 매칭률: {people_match_rate:.1f}%\")\n",
    "\n",
    "            all_data.append(merged_data)\n",
    "\n",
    "            # 메모리 정리\n",
    "            del biz_quarter, people_quarter, merged_data\n",
    "            gc.collect()\n",
    "\n",
    "    if not all_data:\n",
    "        raise ValueError(\"생성된 데이터가 없습니다!\")\n",
    "\n",
    "    # 전체 데이터 결합\n",
    "    print(\"\\n🔗 전체 데이터 결합 중...\")\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"✅ 마스터 데이터셋 생성 완료: {final_df.shape}\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c0c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_fixed(df):\n",
    "    \"\"\"수정된 데이터 전처리\"\"\"\n",
    "    print(\"\\n=== 데이터 전처리 시작 ===\")\n",
    "    print(f\"원본 데이터 크기: {df.shape}\")\n",
    "\n",
    "    # 타겟 변수 확인\n",
    "    if \"당월_매출_금액\" not in df.columns:\n",
    "        raise ValueError(\"타겟 변수 '당월_매출_금액'이 없습니다!\")\n",
    "\n",
    "    # 불필요한 컬럼 제거 + 데이터 리키지 방지\n",
    "    drop_cols = [\"행정동_코드_명\", \"서비스_업종_코드_명\"]\n",
    "\n",
    "    # 🔥 매출 관련 특성 제거 (데이터 리키지 방지)\n",
    "    sales_cols = [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if any(\n",
    "            keyword in col\n",
    "            for keyword in [\n",
    "                \"매출_금액\",\n",
    "                \"매출_건수\",\n",
    "                \"월요일_\",\n",
    "                \"화요일_\",\n",
    "                \"수요일_\",\n",
    "                \"목요일_\",\n",
    "                \"금요일_\",\n",
    "                \"토요일_\",\n",
    "                \"일요일_\",\n",
    "                \"주중_\",\n",
    "                \"주말_\",\n",
    "                \"시간대_\",\n",
    "                \"남성_매출\",\n",
    "                \"여성_매출\",\n",
    "                \"연령대_\",\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 타겟 변수는 유지\n",
    "    sales_cols = [col for col in sales_cols if col != \"당월_매출_금액\"]\n",
    "\n",
    "    print(f\"데이터 리키지 방지를 위해 제거할 매출 관련 컬럼: {len(sales_cols)}개\")\n",
    "    print(f\"제거 컬럼 예시: {sales_cols[:5]}\")\n",
    "\n",
    "    drop_cols.extend(sales_cols)\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns])\n",
    "\n",
    "    # 결측값이 너무 많은 컬럼 제거 (70% 이상)\n",
    "    null_ratio = df.isnull().mean()\n",
    "    high_null_cols = null_ratio[null_ratio > 0.7].index.tolist()\n",
    "    print(f\"결측률 70% 이상 컬럼 제거: {len(high_null_cols)}개\")\n",
    "    if high_null_cols:\n",
    "        df = df.drop(columns=high_null_cols)\n",
    "\n",
    "    # 범주형 변수 인코딩\n",
    "    if \"서비스_업종_코드\" in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[\"서비스_업종_코드_encoded\"] = le.fit_transform(\n",
    "            df[\"서비스_업종_코드\"].astype(str)\n",
    "        )\n",
    "        df = df.drop(columns=[\"서비스_업종_코드\"])\n",
    "\n",
    "    # LocalPeople 관련 컬럼 확인\n",
    "    people_cols = [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if any(keyword in col for keyword in [\"생활인구수\", \"남자\", \"여자\"])\n",
    "    ]\n",
    "    print(f\"📊 LocalPeople 관련 컬럼: {len(people_cols)}개\")\n",
    "\n",
    "    if len(people_cols) == 0:\n",
    "        print(\"⚠️ 경고: LocalPeople 데이터가 포함되지 않았습니다!\")\n",
    "    else:\n",
    "        print(\"✅ LocalPeople 데이터 포함 확인됨\")\n",
    "        # 몇 개 컬럼명 출력\n",
    "        sample_cols = people_cols[:5]\n",
    "        print(f\"   예시: {sample_cols}\")\n",
    "\n",
    "    print(f\"전처리 후 데이터 크기: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d49a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model_fixed(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"수정된 모델 학습 및 평가\"\"\"\n",
    "    print(\"\\n=== 모델 학습 및 평가 ===\")\n",
    "    print(f\"훈련 데이터: {X_train.shape}, 테스트 데이터: {X_test.shape}\")\n",
    "\n",
    "    # 타겟 변수 로그 변환\n",
    "    y_train_log = np.log1p(y_train.clip(lower=0))\n",
    "\n",
    "    # RandomForest 모델\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=15, random_state=42, n_jobs=-1, oob_score=True\n",
    "    )\n",
    "\n",
    "    print(\"🚀 모델 학습 시작...\")\n",
    "    model.fit(X_train, y_train_log)\n",
    "    print(\"✅ 모델 학습 완료!\")\n",
    "\n",
    "    # 예측\n",
    "    print(\"🔮 예측 중...\")\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_pred = np.clip(y_pred, 0, None)  # 음수 제거\n",
    "\n",
    "    # 성능 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎯 수정된 베이스라인 모델 성능 평가 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"MSE:  {mse:,.0f}\")\n",
    "    print(f\"RMSE: {rmse:,.0f} 원\")\n",
    "    print(f\"MAE:  {mae:,.0f} 원\")\n",
    "    print(f\"테스트 데이터 평균 매출: {y_test.mean():,.0f} 원\")\n",
    "    print(f\"Out-of-Bag Score: {model.oob_score_:.4f}\")\n",
    "\n",
    "    # 상위 20개 특성 중요도 출력\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {\"feature\": X_train.columns, \"importance\": model.feature_importances_}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    print(f\"\\n--- 상위 20개 특성 중요도 ---\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(20).iterrows(), 1):\n",
    "        is_people = any(\n",
    "            keyword in row[\"feature\"] for keyword in [\"생활인구수\", \"남자\", \"여자\"]\n",
    "        )\n",
    "        marker = \"🏠\" if is_people else \"🏪\"\n",
    "        print(f\"{i:2d}. {marker} {row['feature'][:50]}: {row['importance']:.4f}\")\n",
    "\n",
    "    # LocalPeople 데이터 기여도\n",
    "    people_features = feature_importance[\n",
    "        feature_importance[\"feature\"].str.contains(\"생활인구수|남자|여자\", regex=True)\n",
    "    ]\n",
    "    people_importance = people_features[\"importance\"].sum()\n",
    "\n",
    "    print(\n",
    "        f\"\\n📊 LocalPeople 데이터 총 기여도: {people_importance:.4f} ({people_importance*100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    if people_importance < 0.05:\n",
    "        print(\"⚠️ LocalPeople 데이터 기여도가 매우 낮습니다.\")\n",
    "    elif people_importance < 0.15:\n",
    "        print(\"🔶 LocalPeople 데이터 기여도가 낮습니다.\")\n",
    "    else:\n",
    "        print(\"✅ LocalPeople 데이터가 유의미하게 기여하고 있습니다.\")\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        {\"mse\": mse, \"rmse\": rmse, \"mae\": mae},\n",
    "        feature_importance,\n",
    "        people_importance,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ac19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_metrics(\n",
    "    model, metrics, feature_importance_df, people_importance, config\n",
    "):\n",
    "    \"\"\"모델과 평가지표 저장\"\"\"\n",
    "\n",
    "    # 결과 저장 디렉토리 생성 (현재 스크립트 디렉토리 기준)\n",
    "    script_dir = os.getcwd()\n",
    "    results_dir = os.path.join(script_dir, \"results\")\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "        print(f\"📁 결과 디렉토리 생성: {results_dir}\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # 1. 모델 저장\n",
    "    model_filename = os.path.join(results_dir, f\"localpeople_model_{timestamp}.joblib\")\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"💾 모델 저장 완료: {model_filename}\")\n",
    "\n",
    "    # 2. 평가지표 CSV 저장\n",
    "    metrics_data = {\n",
    "        \"실행시간\": [timestamp],\n",
    "        \"MSE\": [metrics[\"mse\"]],\n",
    "        \"RMSE\": [metrics[\"rmse\"]],\n",
    "        \"MAE\": [metrics[\"mae\"]],\n",
    "        \"LocalPeople_기여도\": [people_importance],\n",
    "        \"LocalPeople_기여도_퍼센트\": [people_importance * 100],\n",
    "        \"OOB_Score\": [model.oob_score_],\n",
    "        \"테스트_년도\": [config[\"test_year\"]],\n",
    "        \"훈련_데이터_크기\": [config[\"train_size\"]],\n",
    "        \"테스트_데이터_크기\": [config[\"test_size\"]],\n",
    "        \"특성_개수\": [config[\"n_features\"]],\n",
    "    }\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_filename = os.path.join(results_dir, f\"localpeople_metrics_{timestamp}.csv\")\n",
    "    metrics_df.to_csv(metrics_filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"📊 평가지표 CSV 저장 완료: {metrics_filename}\")\n",
    "\n",
    "    # 3. 특성 중요도 저장\n",
    "    importance_filename = os.path.join(\n",
    "        results_dir, f\"localpeople_importance_{timestamp}.csv\"\n",
    "    )\n",
    "    feature_importance_df.to_csv(importance_filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"🔍 특성 중요도 CSV 저장 완료: {importance_filename}\")\n",
    "\n",
    "    # 4. 실행 정보 요약 저장\n",
    "    summary_data = {\n",
    "        \"항목\": [\n",
    "            \"실행시간\",\n",
    "            \"MSE\",\n",
    "            \"RMSE (원)\",\n",
    "            \"MAE (원)\",\n",
    "            \"LocalPeople 기여도 (%)\",\n",
    "            \"OOB Score\",\n",
    "            \"테스트 년도\",\n",
    "            \"훈련 데이터 크기\",\n",
    "            \"테스트 데이터 크기\",\n",
    "            \"특성 개수\",\n",
    "        ],\n",
    "        \"값\": [\n",
    "            timestamp,\n",
    "            f\"{metrics['mse']:,.0f}\",\n",
    "            f\"{metrics['rmse']:,.0f}\",\n",
    "            f\"{metrics['mae']:,.0f}\",\n",
    "            f\"{people_importance*100:.1f}%\",\n",
    "            f\"{model.oob_score_:.4f}\",\n",
    "            config[\"test_year\"],\n",
    "            f\"{config['train_size']:,}\",\n",
    "            f\"{config['test_size']:,}\",\n",
    "            config[\"n_features\"],\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_filename = os.path.join(results_dir, f\"localpeople_summary_{timestamp}.csv\")\n",
    "    summary_df.to_csv(summary_filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"📋 모델 요약 CSV 저장 완료: {summary_filename}\")\n",
    "\n",
    "    print(f\"\\n🎉 모든 결과가 '{results_dir}' 폴더에 저장되었습니다!\")\n",
    "    return {\n",
    "        \"model_file\": model_filename,\n",
    "        \"metrics_file\": metrics_filename,\n",
    "        \"importance_file\": importance_filename,\n",
    "        \"summary_file\": summary_filename,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 수정된 상권 매출 예측 베이스라인 모델\n",
      "==================================================\n",
      "=== 수정된 마스터 데이터셋 생성 시작 ===\n",
      "\n",
      "📅 2019년 데이터 처리 중...\n",
      "  Trading Area 2019 로드 완료: 65666행\n",
      "  🔄 2019년 1분기 처리 중...\n",
      "    2019년 1분기 인구 데이터 로드 중...\n",
      "      201901 완료 (424개 행정동)\n",
      "      201902 완료 (424개 행정동)\n",
      "      201903 완료 (424개 행정동)\n",
      "    2019년 1분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16273 + 인구 424 → 16273행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2019년 2분기 처리 중...\n",
      "    2019년 2분기 인구 데이터 로드 중...\n",
      "      201904 완료 (424개 행정동)\n",
      "      201905 완료 (424개 행정동)\n",
      "      201906 완료 (424개 행정동)\n",
      "    2019년 2분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16418 + 인구 424 → 16418행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2019년 3분기 처리 중...\n",
      "    2019년 3분기 인구 데이터 로드 중...\n",
      "      201907 완료 (424개 행정동)\n",
      "      201908 완료 (424개 행정동)\n",
      "      201909 완료 (424개 행정동)\n",
      "    2019년 3분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16456 + 인구 424 → 16456행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2019년 4분기 처리 중...\n",
      "    2019년 4분기 인구 데이터 로드 중...\n",
      "      201910 완료 (424개 행정동)\n",
      "      201911 완료 (424개 행정동)\n",
      "      201912 완료 (424개 행정동)\n",
      "    2019년 4분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16519 + 인구 424 → 16519행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "\n",
      "📅 2020년 데이터 처리 중...\n",
      "  Trading Area 2020 로드 완료: 66501행\n",
      "  🔄 2020년 1분기 처리 중...\n",
      "    2020년 1분기 인구 데이터 로드 중...\n",
      "      202001 완료 (424개 행정동)\n",
      "      202002 완료 (424개 행정동)\n",
      "      202003 완료 (424개 행정동)\n",
      "    2020년 1분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16502 + 인구 424 → 16502행\n",
      "    📊 인구 데이터 매칭률: 98.6%\n",
      "  🔄 2020년 2분기 처리 중...\n",
      "    2020년 2분기 인구 데이터 로드 중...\n",
      "      202004 완료 (424개 행정동)\n",
      "      202005 완료 (424개 행정동)\n",
      "      202006 완료 (424개 행정동)\n",
      "    2020년 2분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16648 + 인구 424 → 16648행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2020년 3분기 처리 중...\n",
      "    2020년 3분기 인구 데이터 로드 중...\n",
      "      202007 완료 (424개 행정동)\n",
      "      202008 완료 (424개 행정동)\n",
      "      202009 완료 (424개 행정동)\n",
      "    2020년 3분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16676 + 인구 424 → 16676행\n",
      "    📊 인구 데이터 매칭률: 98.6%\n",
      "  🔄 2020년 4분기 처리 중...\n",
      "    2020년 4분기 인구 데이터 로드 중...\n",
      "      202010 완료 (424개 행정동)\n",
      "      202011 완료 (424개 행정동)\n",
      "      202012 완료 (424개 행정동)\n",
      "    2020년 4분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16675 + 인구 424 → 16675행\n",
      "    📊 인구 데이터 매칭률: 98.6%\n",
      "\n",
      "📅 2021년 데이터 처리 중...\n",
      "  Trading Area 2021 로드 완료: 70071행\n",
      "  🔄 2021년 1분기 처리 중...\n",
      "    2021년 1분기 인구 데이터 로드 중...\n",
      "      202101 완료 (424개 행정동)\n",
      "      202102 완료 (424개 행정동)\n",
      "      202103 완료 (424개 행정동)\n",
      "    2021년 1분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17536 + 인구 424 → 17536행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2021년 2분기 처리 중...\n",
      "    2021년 2분기 인구 데이터 로드 중...\n",
      "      202104 완료 (424개 행정동)\n",
      "      202105 완료 (424개 행정동)\n",
      "      202106 완료 (424개 행정동)\n",
      "    2021년 2분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17541 + 인구 424 → 17541행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2021년 3분기 처리 중...\n",
      "    2021년 3분기 인구 데이터 로드 중...\n",
      "      202107 완료 (424개 행정동)\n",
      "      202108 완료 (424개 행정동)\n",
      "      202109 완료 (424개 행정동)\n",
      "    2021년 3분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17514 + 인구 424 → 17514행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2021년 4분기 처리 중...\n",
      "    2021년 4분기 인구 데이터 로드 중...\n",
      "      202110 완료 (424개 행정동)\n",
      "      202111 완료 (424개 행정동)\n",
      "      202112 완료 (424개 행정동)\n",
      "    2021년 4분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17480 + 인구 424 → 17480행\n",
      "    📊 인구 데이터 매칭률: 98.6%\n",
      "\n",
      "📅 2022년 데이터 처리 중...\n",
      "  Trading Area 2022 로드 완료: 69440행\n",
      "  🔄 2022년 1분기 처리 중...\n",
      "    2022년 1분기 인구 데이터 로드 중...\n",
      "      202201 완료 (424개 행정동)\n",
      "      202202 완료 (424개 행정동)\n",
      "      202203 완료 (424개 행정동)\n",
      "    2022년 1분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17394 + 인구 424 → 17394행\n",
      "    📊 인구 데이터 매칭률: 98.6%\n",
      "  🔄 2022년 2분기 처리 중...\n",
      "    2022년 2분기 인구 데이터 로드 중...\n",
      "      202204 완료 (424개 행정동)\n",
      "      202205 완료 (424개 행정동)\n",
      "      202206 완료 (424개 행정동)\n",
      "    2022년 2분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17401 + 인구 424 → 17401행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2022년 3분기 처리 중...\n",
      "    2022년 3분기 인구 데이터 로드 중...\n",
      "      202207 완료 (424개 행정동)\n",
      "      202208 완료 (424개 행정동)\n",
      "      202209 완료 (424개 행정동)\n",
      "    2022년 3분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17346 + 인구 424 → 17346행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2022년 4분기 처리 중...\n",
      "    2022년 4분기 인구 데이터 로드 중...\n",
      "      202210 완료 (424개 행정동)\n",
      "      202211 완료 (424개 행정동)\n",
      "      202212 완료 (424개 행정동)\n",
      "    2022년 4분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17299 + 인구 424 → 17299행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "\n",
      "📅 2023년 데이터 처리 중...\n",
      "  Trading Area 2023 로드 완료: 68643행\n",
      "  🔄 2023년 1분기 처리 중...\n",
      "    2023년 1분기 인구 데이터 로드 중...\n",
      "      202301 완료 (424개 행정동)\n",
      "      202302 완료 (424개 행정동)\n",
      "      202303 완료 (424개 행정동)\n",
      "    2023년 1분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17188 + 인구 424 → 17188행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2023년 2분기 처리 중...\n",
      "    2023년 2분기 인구 데이터 로드 중...\n",
      "      202304 완료 (424개 행정동)\n",
      "      202305 완료 (424개 행정동)\n",
      "      202306 완료 (424개 행정동)\n",
      "    2023년 2분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17196 + 인구 424 → 17196행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2023년 3분기 처리 중...\n",
      "    2023년 3분기 인구 데이터 로드 중...\n",
      "      202307 완료 (424개 행정동)\n",
      "      202308 완료 (424개 행정동)\n",
      "      202309 완료 (424개 행정동)\n",
      "    2023년 3분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17137 + 인구 424 → 17137행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2023년 4분기 처리 중...\n",
      "    2023년 4분기 인구 데이터 로드 중...\n",
      "      202310 완료 (424개 행정동)\n",
      "      202311 완료 (424개 행정동)\n",
      "      202312 완료 (424개 행정동)\n",
      "    2023년 4분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17122 + 인구 424 → 17122행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "\n",
      "📅 2024년 데이터 처리 중...\n",
      "  Trading Area 2024 로드 완료: 67900행\n",
      "  🔄 2024년 1분기 처리 중...\n",
      "    2024년 1분기 인구 데이터 로드 중...\n",
      "      202401 완료 (424개 행정동)\n",
      "      202402 완료 (424개 행정동)\n",
      "      202403 완료 (424개 행정동)\n",
      "    2024년 1분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17044 + 인구 424 → 17044행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2024년 2분기 처리 중...\n",
      "    2024년 2분기 인구 데이터 로드 중...\n",
      "      202404 완료 (424개 행정동)\n",
      "      202405 완료 (424개 행정동)\n",
      "      202406 완료 (424개 행정동)\n",
      "    2024년 2분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 17048 + 인구 424 → 17048행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2024년 3분기 처리 중...\n",
      "    2024년 3분기 인구 데이터 로드 중...\n",
      "      202407 완료 (424개 행정동)\n",
      "      202408 완료 (424개 행정동)\n",
      "      202409 완료 (424개 행정동)\n",
      "    2024년 3분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16937 + 인구 424 → 16937행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "  🔄 2024년 4분기 처리 중...\n",
      "    2024년 4분기 인구 데이터 로드 중...\n",
      "      202410 완료 (424개 행정동)\n",
      "      202411 완료 (424개 행정동)\n",
      "      202412 완료 (424개 행정동)\n",
      "    2024년 4분기 인구 데이터 집계 완료: 424개 행정동\n",
      "    ✅ 병합 완료: 매출 16871 + 인구 424 → 16871행\n",
      "    📊 인구 데이터 매칭률: 98.5%\n",
      "\n",
      "🔗 전체 데이터 결합 중...\n",
      "✅ 마스터 데이터셋 생성 완료: (408221, 82)\n",
      "\n",
      "=== 데이터 전처리 시작 ===\n",
      "원본 데이터 크기: (408221, 82)\n",
      "데이터 리키지 방지를 위해 제거할 매출 관련 컬럼: 47개\n",
      "제거 컬럼 예시: ['당월_매출_건수', '주중_매출_금액', '주말_매출_금액', '월요일_매출_금액', '화요일_매출_금액']\n",
      "결측률 70% 이상 컬럼 제거: 0개\n",
      "📊 LocalPeople 관련 컬럼: 29개\n",
      "✅ LocalPeople 데이터 포함 확인됨\n",
      "   예시: ['총생활인구수', '남자0세부터9세생활인구수', '남자10세부터14세생활인구수', '남자15세부터19세생활인구수', '남자20세부터24세생활인구수']\n",
      "전처리 후 데이터 크기: (408221, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 수정된 상권 매출 예측 베이스라인 모델\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 설정\n",
    "YEARS = range(2019, 2025)\n",
    "PEOPLE_DIR = \"../../Data/LocalPeople\"\n",
    "BIZ_DIR = \"../../Data/Trading_Area\"\n",
    "TEST_YEAR = 2024\n",
    "\n",
    "# 1. 데이터 로드 및 병합 (수정된 버전)\n",
    "master_df = create_master_dataset_fixed(YEARS, PEOPLE_DIR, BIZ_DIR)\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "processed_df = preprocess_data_fixed(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f27d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 데이터 분할 (2024년 테스트) ===\n",
      "훈련 데이터: 340321행\n",
      "테스트 데이터: 67900행\n"
     ]
    }
   ],
   "source": [
    "# 3. 데이터 분할\n",
    "print(f\"\\n=== 데이터 분할 ({TEST_YEAR}년 테스트) ===\")\n",
    "train_mask = processed_df[\"기준_년분기_코드\"] < (TEST_YEAR * 10 + 1)\n",
    "test_mask = processed_df[\"기준_년분기_코드\"] >= (TEST_YEAR * 10 + 1)\n",
    "\n",
    "train_df = processed_df[train_mask].copy()\n",
    "test_df = processed_df[test_mask].copy()\n",
    "\n",
    "print(f\"훈련 데이터: {len(train_df)}행\")\n",
    "print(f\"테스트 데이터: {len(test_df)}행\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81345844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측값 처리 중...\n"
     ]
    }
   ],
   "source": [
    "# 특성과 타겟 분리\n",
    "target_col = \"당월_매출_금액\"\n",
    "feature_cols = [col for col in train_df.columns if col != target_col]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# 결측값 처리\n",
    "print(\"결측값 처리 중...\")\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train_filled = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_filled = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31420ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 모델 학습 및 평가 ===\n",
      "훈련 데이터: (340321, 32), 테스트 데이터: (67900, 32)\n",
      "🚀 모델 학습 시작...\n",
      "✅ 모델 학습 완료!\n",
      "🔮 예측 중...\n",
      "\n",
      "============================================================\n",
      "🎯 수정된 베이스라인 모델 성능 평가 결과\n",
      "============================================================\n",
      "MSE:  89,297,344,592,772,399,104\n",
      "RMSE: 9,449,727,223 원\n",
      "MAE:  826,261,234 원\n",
      "테스트 데이터 평균 매출: 1,532,612,027 원\n",
      "Out-of-Bag Score: 0.6895\n",
      "\n",
      "--- 상위 20개 특성 중요도 ---\n",
      " 1. 🏪 서비스_업종_코드_encoded: 0.5833\n",
      " 2. 🏪 행정동코드: 0.0478\n",
      " 3. 🏠 여자25세부터29세생활인구수: 0.0404\n",
      " 4. 🏠 남자30세부터34세생활인구수: 0.0345\n",
      " 5. 🏠 남자35세부터39세생활인구수: 0.0231\n",
      " 6. 🏠 남자50세부터54세생활인구수: 0.0164\n",
      " 7. 🏠 여자30세부터34세생활인구수: 0.0156\n",
      " 8. 🏠 여자10세부터14세생활인구수: 0.0155\n",
      " 9. 🏠 여자20세부터24세생활인구수: 0.0138\n",
      "10. 🏠 남자0세부터9세생활인구수: 0.0137\n",
      "11. 🏠 남자25세부터29세생활인구수: 0.0132\n",
      "12. 🏠 남자45세부터49세생활인구수: 0.0132\n",
      "13. 🏠 남자40세부터44세생활인구수: 0.0131\n",
      "14. 🏠 여자0세부터9세생활인구수: 0.0125\n",
      "15. 🏠 여자15세부터19세생활인구수: 0.0116\n",
      "16. 🏠 여자70세이상생활인구수: 0.0116\n",
      "17. 🏠 남자15세부터19세생활인구수: 0.0113\n",
      "18. 🏠 남자10세부터14세생활인구수: 0.0106\n",
      "19. 🏪 기준_년분기_코드: 0.0095\n",
      "20. 🏠 남자20세부터24세생활인구수: 0.0093\n",
      "\n",
      "📊 LocalPeople 데이터 총 기여도: 0.3595 (35.9%)\n",
      "✅ LocalPeople 데이터가 유의미하게 기여하고 있습니다.\n",
      "\n",
      "🎉 수정된 베이스라인 모델 학습 완료!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. 모델 학습 및 평가\n",
    "model, metrics, feature_importance, people_importance = train_and_evaluate_model_fixed(\n",
    "    X_train_filled, y_train, X_test_filled, y_test\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 수정된 베이스라인 모델 학습 완료!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ed32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 모델 저장 완료: /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/Code/Analyze_with_LocalPeople/results/localpeople_model_20250619_212155.joblib\n",
      "📊 평가지표 CSV 저장 완료: /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/Code/Analyze_with_LocalPeople/results/localpeople_metrics_20250619_212155.csv\n",
      "🔍 특성 중요도 CSV 저장 완료: /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/Code/Analyze_with_LocalPeople/results/localpeople_importance_20250619_212155.csv\n",
      "📋 모델 요약 CSV 저장 완료: /Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/Code/Analyze_with_LocalPeople/results/localpeople_summary_20250619_212155.csv\n",
      "\n",
      "🎉 모든 결과가 '/Users/rubidium/Project/BDP/test/BigDataProgramming_TeamI/Code/Analyze_with_LocalPeople/results' 폴더에 저장되었습니다!\n",
      "\n",
      "🎉 모든 결과가 저장되었습니다!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. 모델 및 결과 저장\n",
    "config = {\n",
    "    \"test_year\": TEST_YEAR,\n",
    "    \"train_size\": len(train_df),\n",
    "    \"test_size\": len(test_df),\n",
    "    \"n_features\": len(feature_cols),\n",
    "}\n",
    "result = save_model_and_metrics(\n",
    "    model, metrics, feature_importance, people_importance, config\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 모든 결과가 저장되었습니다!\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
